"""
AION Async Streaming Responses
Provides streaming output for LLM responses.
Auto-generated by self-development cycle.
"""

import asyncio
from typing import AsyncIterator, Callable, Any
from dataclasses import dataclass

@dataclass
class StreamChunk:
    content: str
    done: bool = False
    metadata: dict = None

class StreamingResponse:
    """
    Async streaming response handler.
    Enables real-time token streaming from LLMs.
    """
    
    def __init__(self):
        self._buffer: list[str] = []
        self._done = False
        self._callbacks: list[Callable] = []
        
    def on_chunk(self, callback: Callable[[StreamChunk], None]):
        """Register callback for new chunks."""
        self._callbacks.append(callback)
        
    async def write(self, content: str):
        """Write a chunk to the stream."""
        self._buffer.append(content)
        chunk = StreamChunk(content=content)
        
        for callback in self._callbacks:
            if asyncio.iscoroutinefunction(callback):
                await callback(chunk)
            else:
                callback(chunk)
    
    async def close(self):
        """Close the stream."""
        self._done = True
        chunk = StreamChunk(content="", done=True)
        
        for callback in self._callbacks:
            if asyncio.iscoroutinefunction(callback):
                await callback(chunk)
            else:
                callback(chunk)
    
    def get_full_response(self) -> str:
        """Get the complete buffered response."""
        return "".join(self._buffer)
    
    @property
    def is_done(self) -> bool:
        return self._done


async def stream_local_response(prompt: str, engine) -> AsyncIterator[StreamChunk]:
    """
    Stream a local reasoning response.
    Simulates token-by-token streaming.
    """
    # Get full response from local engine
    response = engine.decide(prompt)['decision']
    
    # Stream word by word
    words = response.split()
    for i, word in enumerate(words):
        yield StreamChunk(
            content=word + " ",
            done=(i == len(words) - 1)
        )
        await asyncio.sleep(0.02)  # Simulate typing effect


async def stream_cloud_response(prompt: str, provider: str = "openai") -> AsyncIterator[StreamChunk]:
    """
    Stream from a cloud LLM provider.
    Placeholder for actual API integration.
    """
    # Simulated streaming response
    response = f"[Streaming from {provider}] Processing: {prompt[:50]}..."
    
    for char in response:
        yield StreamChunk(content=char, done=False)
        await asyncio.sleep(0.01)
    
    yield StreamChunk(content="", done=True)


class StreamingAgent:
    """
    Agent wrapper that supports streaming output.
    """
    
    def __init__(self, agent_runner):
        self.runner = agent_runner
        
    async def run_streaming(self, agent_name: str, input_data: str) -> AsyncIterator[StreamChunk]:
        """Run agent with streaming output."""
        from src.runtime.local_engine import LocalReasoningEngine
        engine = LocalReasoningEngine()
        
        # Stream the response
        async for chunk in stream_local_response(input_data, engine):
            yield chunk


# Demo function
async def demo_streaming():
    """Demonstrate streaming responses."""
    from src.runtime.local_engine import LocalReasoningEngine
    
    print("ðŸŒŠ Streaming Response Demo")
    print("-" * 40)
    print("Response: ", end="", flush=True)
    
    engine = LocalReasoningEngine()
    
    async for chunk in stream_local_response("What is AION?", engine):
        print(chunk.content, end="", flush=True)
        
    print("\n" + "-" * 40)
    print("âœ… Streaming complete!")


if __name__ == "__main__":
    asyncio.run(demo_streaming())
